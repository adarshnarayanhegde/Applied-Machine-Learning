{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tg9fRSjFtsRu"
   },
   "source": [
    "# Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jjsZpx5C9eBH"
   },
   "source": [
    "More often than not, we will use a deep learning library (Tensorflow, Pytorch, or the wrapper known as Keras) to implement our models. However, the abstraction afforded by those libraries can make it hard to troubleshoot issues if we don't understand what is going on under the hood. \n",
    "\n",
    "### In this section we will implement a fully-connected and a convolutional neural network from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2NzW9M-btzqO"
   },
   "source": [
    "##### The neural network will have the following architecture:\n",
    "\n",
    "- Input layer\n",
    "- Dense hidden layer with 512 neurons, using relu as the activation function\n",
    "- Dropout with a value of 0.2\n",
    "- Dense hidden layer with 512 neurons, using relu as the activation function\n",
    "- Dropout with a value of 0.2\n",
    "- Output layer, using softmax as the activation function\n",
    "\n",
    "The model will use categorical crossentropy as its loss function. \n",
    "We will optimize the gradient descent using RMSProp, with a learning rate of 0.001 and a rho value of 0.9.\n",
    "We will evaluate the model using accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8rPUmRqBtpS2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self, epochs, drop, learning_rate, rho, mini_batch, ip_dimension, op_dimension):\n",
    "        self.epochs=epochs\n",
    "        self.learning_rate=learning_rate\n",
    "        self.mini_batch=mini_batch\n",
    "        self.ip_dimension=ip_dimension\n",
    "        self.op_dimension=op_dimension\n",
    "        self.drop=drop\n",
    "        self.rho=rho\n",
    "        self.neurons=512\n",
    "        self.bias_1=np.zeros((1,self.neurons))\n",
    "        self.weight_1=np.random.randn(self.ip_dimension,self.neurons)\n",
    "        self.bias_2=np.zeros((1,self.neurons))\n",
    "        self.weight_2=np.random.randn(self.neurons,self.neurons)\n",
    "        self.bias_3=np.zeros((1,self.op_dimension))\n",
    "        self.weight_3=np.random.randn(self.neurons,self.op_dimension)\n",
    "\n",
    "\n",
    "\n",
    "    #ReLu Activation Function\n",
    "    def relu(self,z):\n",
    "        return np.maximum(0,z)\n",
    "\n",
    "    #Derivative of ReLu Activation\n",
    "    def relu_derivative(self,z):\n",
    "        z[z<=0] = 0\n",
    "        z[z>0] = 1\n",
    "        return z\n",
    "\n",
    "    #Softmax Activation Function\n",
    "    def softmax(self,z):\n",
    "        z=z-np.max(z,axis=1,keepdims=True)\n",
    "        return np.exp(z)/np.sum(np.exp(z),axis=1,keepdims=True)\n",
    "\n",
    "    #RMSProp to optimize gradient descent\n",
    "    def rmsprop(self,x,del_x,rho=0.9):\n",
    "        ep=1e-8\n",
    "        w=np.zeros(x.shape, dtype=float)\n",
    "        w=rho * w + (1-rho) * del_x ** 2\n",
    "        x=x-self.learning_rate * (del_x / (np.sqrt(w)+ep))\n",
    "        return x\n",
    "\n",
    "    #Cross entropy loss\n",
    "    def cross_entropy(self,out_3,y):\n",
    "        entropy = -np.mean(y * np.log(out_3 + 1e-8))\n",
    "        return entropy\n",
    "\n",
    "    #Error function\n",
    "    def error(self,predicted, real):\n",
    "        samples = real.shape[0]\n",
    "        res = predicted - real\n",
    "        return res/samples\n",
    "\n",
    "\n",
    "    def fit(self,x_train,y_train):\n",
    "\n",
    "        print(\"Model Fit...\")\n",
    "        for i in range(self.epochs):\n",
    "            #print(\"Epoch:\",i+1)\n",
    "            start=0\n",
    "            end=self.mini_batch\n",
    "            while end<=x_train.shape[0]:\n",
    "\n",
    "                #FeedForward\n",
    "                x=x_train[start:end:]\n",
    "                y=y_train[start:end:]\n",
    "\n",
    "                net_1=np.matmul(x,self.weight_1)+self.bias_1\n",
    "                out_1=self.relu(net_1)\n",
    "\n",
    "                drop_1 = np.random.binomial(1, (1-self.drop), size=out_1.shape)/(1-self.drop)\n",
    "                out_1*=drop_1\n",
    "\n",
    "                net_2=np.matmul(out_1,self.weight_2)+self.bias_2\n",
    "                out_2=self.relu(net_2)\n",
    "\n",
    "                drop_2 = np.random.binomial(1, (1-self.drop), size=out_2.shape)/(1-self.drop)\n",
    "                out_2*=drop_2\n",
    "\n",
    "                net_3=np.matmul(out_2,self.weight_3)+self.bias_3\n",
    "                out_3=self.softmax(net_3)\n",
    "\n",
    "                cross_entr = self.cross_entropy(out_3,y)\n",
    "                #print(\"Cross Entropy Loss:\",cross_entr)\n",
    "\n",
    "                #Backprop\n",
    "\n",
    "                del_a3=self.error(out_3,y)\n",
    "                del_weight_3=np.matmul(out_2.T,del_a3)\n",
    "                del_bias_3=del_a3\n",
    "\n",
    "                del_inter2= np.matmul(del_a3,self.weight_3.T)\n",
    "                del_a2= del_inter2 * self.relu_derivative(out_2)\n",
    "                del_weight_2=np.matmul(out_1.T,del_a2)\n",
    "                del_bias_2=del_a2\n",
    "\n",
    "                del_inter1=np.matmul(del_a2,self.weight_2.T)\n",
    "                del_a1= del_inter1 * self.relu_derivative(out_1)\n",
    "                del_weight_1=np.matmul(x.T,del_a1)\n",
    "                del_bias_1=del_a1\n",
    "\n",
    "                self.weight_3=self.rmsprop(self.weight_3,del_weight_3,self.rho)\n",
    "                self.bias_3=self.rmsprop(self.bias_3,del_bias_3,self.rho)\n",
    "\n",
    "                self.weight_2=self.rmsprop(self.weight_2,del_weight_2,self.rho)\n",
    "                self.bias_2=self.rmsprop(self.bias_2,del_bias_2,self.rho)\n",
    "\n",
    "                self.weight_1=self.rmsprop(self.weight_1,del_weight_1,self.rho)\n",
    "                self.bias_1=self.rmsprop(self.bias_1,del_bias_1,self.rho)\n",
    "\n",
    "                start+=self.mini_batch\n",
    "                end+=self.mini_batch\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def evaluate(self,x_test,y_test):\n",
    "        print(\"Model Evaluate...\")\n",
    "        res=[]\n",
    "        for i in range(20):\n",
    "            #print(\"Epoch:\",i+1)\n",
    "            res=[]\n",
    "            start=0\n",
    "            end=self.mini_batch\n",
    "            while end<=x_test.shape[0]:\n",
    "\n",
    "                x=x_test[start:end:]\n",
    "\n",
    "                net_1=np.matmul(x,self.weight_1)+self.bias_1\n",
    "                out_1=self.relu(net_1)\n",
    "\n",
    "                drop_1 = np.random.binomial(1, (1-self.drop), size=out_1.shape)/(1-self.drop)\n",
    "                out_1*=drop_1\n",
    "\n",
    "                net_2=np.matmul(out_1,self.weight_2)+self.bias_2\n",
    "                out_2=self.relu(net_2)\n",
    "\n",
    "                drop_2 = np.random.binomial(1, (1-self.drop), size=out_2.shape)/(1-self.drop)\n",
    "                out_2*=drop_2\n",
    "\n",
    "                net_3=np.matmul(out_2,self.weight_3)+self.bias_3\n",
    "                out_3=self.softmax(net_3)\n",
    "\n",
    "                res.append(np.argmax(out_3,axis=1))\n",
    "\n",
    "                start+=self.mini_batch\n",
    "                end+=self.mini_batch\n",
    "\n",
    "\n",
    "        flat_res=[]\n",
    "        for i in res:\n",
    "            flat_res.extend(i)\n",
    "\n",
    "        flat_res=np.asarray(flat_res)\n",
    "        actual=np.argmax(y_test,axis=1)\n",
    "\n",
    "        accuracy = (actual == flat_res).mean()\n",
    "        #print(\"Acccuracy:\",accuracy * 100)\n",
    "        return accuracy * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DH3bgJyPuE2O"
   },
   "source": [
    "##### Let us train our fully-connected neural network on the Fashion-MNIST dataset using 5-fold cross validation and report accuracy on the folds, as well as on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XsN4sUoUugl8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [12000 12001 12002 ... 59997 59998 59999] Test: [    0     1     2 ... 11997 11998 11999]\n",
      "Model Fit...\n",
      "Model Evaluate...\n",
      "Train: [    0     1     2 ... 59997 59998 59999] Test: [12000 12001 12002 ... 23997 23998 23999]\n",
      "Model Fit...\n",
      "Model Evaluate...\n",
      "Train: [    0     1     2 ... 59997 59998 59999] Test: [24000 24001 24002 ... 35997 35998 35999]\n",
      "Model Fit...\n",
      "Model Evaluate...\n",
      "Train: [    0     1     2 ... 59997 59998 59999] Test: [36000 36001 36002 ... 47997 47998 47999]\n",
      "Model Fit...\n",
      "Model Evaluate...\n",
      "Train: [    0     1     2 ... 47997 47998 47999] Test: [48000 48001 48002 ... 59997 59998 59999]\n",
      "Model Fit...\n",
      "Model Evaluate...\n",
      "\n",
      "Validation Accuracies:\n",
      "Fold 1 Accuracy:  77.80833333333334\n",
      "Fold 2 Accuracy:  85.175\n",
      "Fold 3 Accuracy:  88.94999999999999\n",
      "Fold 4 Accuracy:  89.56666666666668\n",
      "Fold 5 Accuracy:  89.86666666666666\n",
      "Model Evaluate...\n",
      "Test Accuracy:  81.37\n"
     ]
    }
   ],
   "source": [
    "# To simplify the usage of our dataset, we will be importing it from the Keras \n",
    "# library. Keras can be installed using pip: python -m pip install keras\n",
    "\n",
    "# Original source for the dataset:\n",
    "# https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "# Reference to the Fashion-MNIST's Keras function: \n",
    "# https://keras.io/datasets/#fashion-mnist-database-of-fashion-articles\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train= preprocessing.normalize(x_train)\n",
    "y_train=preprocessing.normalize(y_train)\n",
    "\n",
    "x_test= preprocessing.normalize(x_test)\n",
    "y_test= preprocessing.normalize(y_test)\n",
    "\n",
    "\n",
    "#Initializing Neural Network\n",
    "neuNet=NeuralNetwork(20,0.2,0.001,0.9,1000,x_train.shape[1],y_train.shape[1])\n",
    "\n",
    "#5-Fold Cross Validation\n",
    "kf=KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "fold_acc=[]\n",
    "for train_index, test_index in kf.split(x_train):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    x_train_fold, x_test_fold = x_train[train_index], x_train[test_index]\n",
    "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "    neuNet.fit(x_train_fold,y_train_fold)\n",
    "    accuracy=neuNet.evaluate(x_test_fold,y_test_fold)\n",
    "    fold_acc.append(accuracy)\n",
    "\n",
    "\n",
    "print(\"\\nValidation Accuracies:\")\n",
    "for i in range(len(fold_acc)):\n",
    "    print(\"Fold %d Accuracy: \"%(i+1),fold_acc[i])\n",
    "\n",
    "#Testing Model\n",
    "accuracy=neuNet.evaluate(x_test,y_test)\n",
    "print(\"\\nTest Accuracy: \",accuracy)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A3-P556-F19.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
